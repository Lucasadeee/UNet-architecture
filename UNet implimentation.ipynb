{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5a5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc055018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "864785de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input images for each stage being passed through two convolutional layers of 64, 128, 256 and 512 channels as indicated in the image of the Unet structure\n",
    "def double_conv(in_c, out_c):\n",
    "    conv=nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    return conv\n",
    "\n",
    "#each output image structure after going through two conolutional layers is being copied by cropping in terms of size to resemble a copy of the downsampled image, another method this could have been done is through padding the downsampled images. However, the original UNet by Olaf Ronneberger et al. model does not use this\n",
    "def crop_img(tensor, target_tensor):\n",
    "    target_size=target_tensor.size()[2]\n",
    "    target_size=tensor.size()[2]\n",
    "    delta=tensor_size-target_size\n",
    "    delta=delta//2\n",
    "    return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f5e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.max_pool_2x2=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.down_conv_1=double_conv(1, 64)\n",
    "        self.down_conv_2=double_conv(64, 128)\n",
    "        self.down_conv_3=double_conv(128, 256)\n",
    "        self.down_conv_4=double_conv(256, 512)\n",
    "        self.down_conv_5=double_conv(512, 1024)\n",
    "    #downsampling stages\n",
    "        \n",
    "        self.up_trans_1=nn.ConvTranspose2d(\n",
    "            in_channels= 1024, \n",
    "            out_channels=512, kernel_size=2,\n",
    "            stride=2)\n",
    "        self.up_conv_1=double_conv(1024, 512)\n",
    "        \n",
    "        \n",
    "        self.up_trans_2=nn.ConvTranspose2d(\n",
    "            in_channels= 512, \n",
    "            out_channels=256, kernel_size=2,\n",
    "            stride=2)\n",
    "        self.up_conv_2=double_conv(512, 256)\n",
    "        \n",
    "        \n",
    "        self.up_trans_3=nn.ConvTranspose2d(\n",
    "            in_channels= 256, \n",
    "            out_channels=128, kernel_size=2,\n",
    "            stride=2)\n",
    "        self.up_conv_3=double_conv(256, 128)\n",
    "        \n",
    "        self.up_trans_4=nn.ConvTranspose2d(\n",
    "            in_channels= 128, \n",
    "            out_channels=64, kernel_size=2,\n",
    "            stride=2)\n",
    "        self.up_conv_4=double_conv(128, 64)\n",
    "        \n",
    "        self.out=nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=2, kernel_size=1\n",
    "        )\n",
    "        \n",
    "    #Upsampling stages also called by some as deconvolution or up-convolution. Here Transposed convolution is used for compiling the upsampling. Another way of upsampling compilation is using Bilinear interpolation, However, the original UNet by Olaf Ronneberger et al. does not use this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a85abe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, image):\n",
    "    #batch_size,  channel, height, width\n",
    "    #encoder stage using simple sonvolutional neural network architecture\n",
    "    x1=self.down_conv_1(image)#input image stage 1\n",
    "    x2=self.max_pool_2x2(x1)\n",
    "    x3=self.down_conv_2(x2)#input image stage 2\n",
    "    x4=self.max_pool_2x2(x3)\n",
    "    x5=self.down_conv_3(x4)#input image stage 3\n",
    "    x6=self.max_pool_2x2(x5)\n",
    "    x7=self.down_conv_4(x6)#input image stage 4\n",
    "    x8=self.max_pool_2x2(x7)\n",
    "    x9=self.down_conv_5(x8)\n",
    "\n",
    "    #Decoder\n",
    "    #Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution\n",
    "    x=self.up_trans_1(x9)\n",
    "    y=crop_img(x7, x)\n",
    "    x=self.up_conv_1(torch.cat([x,y], 1))\n",
    "    print(x.size())\n",
    "    \n",
    "    x=self.up_trans_2(x)\n",
    "    y=crop_img(x5, x)\n",
    "    x=self.up_conv_2(torch.cat([x,y], 1))\n",
    "    \n",
    "    x=self.up_trans_3(x)\n",
    "    y=crop_img(x3, x)\n",
    "    x=self.up_conv_3(torch.cat([x,y], 1))\n",
    "    \n",
    "    x=self.up_trans_4(x)\n",
    "    y=crop_img(x1, x)\n",
    "    x=self.up_conv_4(torch.cat([x,y], 1))\n",
    "    \n",
    "    x=self.out(x)\n",
    "    \n",
    "    print (x.size())\n",
    "    return x\n",
    "\n",
    "    \n",
    "    if __name__==\"__main__\":\n",
    "        image=torch.rand((1,1,572,572))\n",
    "        model=UNet()\n",
    "        print(model(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20585644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The encoder stage can be changed and adjusted based on another pre trained model that would be preferred to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb2f907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
